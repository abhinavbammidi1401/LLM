{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNau9wXOvivHIUqLPXzCc47",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinavbammidi1401/LLM/blob/main/2348509_Lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfx9IGZ6qgAn",
        "outputId": "931d565a-f95d-411a-e58b-6cda7ab92eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m885.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch torchvision sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "text = input(\"Enter the text: \")\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  embeddings = model(**inputs).last_hidden_state\n",
        "\n",
        "sentence_embedding = embeddings.mean(dim=1)\n",
        "print(\"Sentence Embedding: \", sentence_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXZecGoIr64Y",
        "outputId": "c6b937d7-625c-4822-bbad-66b668b1604d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text: hello\n",
            "Sentence Embedding:  tensor([[-3.7831e-01,  3.3123e-01,  3.1439e-01,  5.1704e-01, -4.9871e-01,\n",
            "         -4.4944e-01,  4.1317e-01,  1.1087e-01, -4.9427e-01, -2.2531e-01,\n",
            "          7.3075e-02,  2.1204e-02, -2.4917e-02, -2.6388e-01,  1.3143e-01,\n",
            "         -3.0753e-02,  1.1780e-01, -2.5523e-01, -6.6512e-01,  3.2693e-02,\n",
            "         -3.3590e-01,  1.6907e-01, -1.3957e-01,  1.7165e-01, -3.2370e-01,\n",
            "         -3.1702e-01,  2.0455e-01,  2.7355e-01,  1.4295e-01, -4.4068e-01,\n",
            "          3.3013e-01,  1.0274e-01,  4.9034e-01, -1.7253e-02,  7.2069e-02,\n",
            "          4.4332e-01, -5.6795e-01, -4.9035e-01,  2.4116e-01,  4.1718e-03,\n",
            "         -8.0719e-02, -3.2869e-01,  3.1047e-02, -1.5754e-01,  2.2183e-01,\n",
            "         -2.0467e-01,  1.2712e-01,  3.3719e-01,  3.4824e-01, -3.2656e-02,\n",
            "         -4.1231e-01, -5.4384e-01, -2.5835e-01,  1.4255e-01,  7.3222e-01,\n",
            "          1.9523e-01, -1.3656e-01, -1.4031e-01,  2.9451e-01, -3.5770e-01,\n",
            "         -2.0531e-01,  2.1674e-01, -4.8912e-01, -1.3247e-01,  7.9481e-02,\n",
            "         -2.7636e-01, -4.2441e-01, -3.1703e-01, -2.8946e-01, -4.4927e-01,\n",
            "         -1.7316e-01,  7.7335e-02, -2.5868e-01,  3.2342e-02, -2.3563e-01,\n",
            "          9.5942e-02,  1.2200e-01,  3.1741e-02,  5.7745e-02, -2.8734e-02,\n",
            "          2.8981e-01, -4.4054e-01, -3.0342e-01,  5.8177e-02,  8.1341e-02,\n",
            "          2.5939e-03,  1.1888e-01,  3.7665e-01, -1.0985e-01,  1.8742e-01,\n",
            "         -5.3550e-01,  2.9008e-01,  1.5714e-01, -6.9272e-02, -5.9883e-01,\n",
            "         -1.6476e-01,  4.5933e-01, -1.2636e-02, -7.4485e-01,  1.7964e+00,\n",
            "          3.3935e-01,  4.7298e-01,  6.6557e-02,  3.1580e-01, -7.1599e-03,\n",
            "          5.8199e-03, -3.2742e-01,  1.6131e-01, -7.3492e-02,  6.4590e-02,\n",
            "         -2.2351e-04, -2.1435e-01, -1.9337e-01,  9.7446e-02,  5.2282e-01,\n",
            "          1.0932e-01, -6.6401e-02,  2.9978e-01,  1.3609e-01, -1.3477e-04,\n",
            "          9.0035e-02, -6.7357e-02,  3.4108e-02, -2.4799e-02,  1.9181e-02,\n",
            "          1.4383e-01,  1.0879e-01, -3.5260e-32,  3.3815e-01, -2.0792e-01,\n",
            "          1.9956e-01,  1.0108e+00, -1.8706e-01, -2.9142e-02, -3.6803e-01,\n",
            "         -3.7795e-01,  1.6529e-01,  3.8355e-01,  2.6120e-01,  3.6661e-01,\n",
            "         -1.0945e-01,  2.5797e-01,  1.1465e-01,  5.3022e-01, -2.3585e-01,\n",
            "          2.6604e-01, -2.9779e-02,  3.0768e-01, -3.2737e-01,  6.7338e-02,\n",
            "          1.6091e-01,  4.5259e-01,  2.9345e-01, -2.6242e-01,  8.0390e-02,\n",
            "         -6.1853e-01,  3.1434e-01,  1.3399e-01, -1.7925e-01, -2.5476e-01,\n",
            "          1.3852e-01,  2.3851e-01,  5.4812e-02,  1.2575e-01,  3.0692e-02,\n",
            "         -3.7827e-01, -3.0265e-01, -3.1490e-02, -3.2242e-01,  1.7944e-01,\n",
            "          1.3111e-01, -1.2462e-01,  1.4384e-01,  3.4996e-02, -7.3152e-03,\n",
            "          1.3657e-01,  1.9025e-02,  1.8659e-01, -3.1923e-01,  1.1256e-01,\n",
            "         -8.4468e-01,  2.4998e-01, -6.1945e-02, -6.9869e-02, -2.0165e-01,\n",
            "         -3.0489e-01,  2.8246e-01,  1.4895e-01,  2.0188e-01,  6.7324e-01,\n",
            "         -2.4317e-01, -2.5822e-02, -4.8663e-01, -3.3818e-01,  2.3106e-01,\n",
            "          6.9359e-02,  4.1427e-01, -2.2973e-01, -2.7716e-01, -9.9078e-02,\n",
            "          1.4713e-01,  7.0647e-02,  4.9230e-02,  2.3527e-01,  1.5782e-01,\n",
            "          6.3172e-02,  2.5831e-01, -2.7953e-01,  3.9060e-02,  2.6773e-01,\n",
            "         -1.1353e-01,  4.3775e-02,  3.3858e-01,  3.2230e-01, -1.2948e-01,\n",
            "         -5.0918e-01, -7.7227e-02, -2.3791e-01, -3.4971e-01,  1.8884e-01,\n",
            "          2.7372e-01,  7.1352e-02, -1.0792e-01,  2.7640e-32,  7.9257e-01,\n",
            "          4.7809e-01, -5.7230e-01, -1.4749e-01, -3.3639e-01, -5.5117e-02,\n",
            "         -1.9340e-01,  6.8581e-01, -8.7037e-01,  5.1286e-02,  1.8482e-01,\n",
            "         -7.4822e-02,  4.2290e-01,  1.6983e-01,  2.4603e-01,  1.1807e-01,\n",
            "          8.6153e-01,  3.4274e-01, -2.4181e-01, -1.0754e-01, -3.6995e-01,\n",
            "          5.2800e-03, -3.3091e-01, -4.2805e-02, -2.9782e-03, -7.9068e-02,\n",
            "         -1.3663e-02,  3.5297e-01, -6.0096e-01, -1.5388e-01,  4.7349e-01,\n",
            "          1.2366e-01, -2.7925e-02,  1.8263e-01,  1.0145e-01,  5.5108e-01,\n",
            "          1.0107e-01, -4.8096e-01,  2.5620e-01, -5.0865e-01, -1.4379e-01,\n",
            "          2.8338e-01,  1.4554e-02,  6.5884e-01, -2.0434e-01, -3.8584e-01,\n",
            "         -2.2735e-01,  1.7523e-01, -2.5583e-01,  9.5596e-02, -5.4607e-01,\n",
            "         -3.3609e-01,  1.3626e-01,  4.8216e-02, -1.3542e-01,  1.3294e-01,\n",
            "         -1.5187e-01,  1.8285e-01,  6.0936e-02, -1.1178e-01,  1.0239e-01,\n",
            "          4.5991e-01,  2.5084e-01,  5.2747e-01, -7.2881e-02,  1.8765e-01,\n",
            "         -1.9401e-01,  7.7131e-02,  8.1596e-02, -1.7931e-01,  2.2173e-01,\n",
            "         -3.5233e-02, -6.6629e-02,  2.3306e-01, -1.2500e-01, -6.7892e-02,\n",
            "         -1.4324e-01, -6.0175e-02, -1.3927e-01,  7.2390e-02, -6.4153e-02,\n",
            "          3.0954e-01, -1.6715e-01, -1.2740e-03,  5.7032e-03, -1.9034e-01,\n",
            "          3.0875e-01,  2.6716e-01, -2.2742e-02, -2.5113e-01,  1.7450e-01,\n",
            "          1.9925e-01, -9.1513e-02, -9.5010e-04, -2.6593e-01, -8.9218e-08,\n",
            "         -5.2409e-02,  7.8415e-04,  9.9268e-02,  3.5696e-01,  2.7445e-01,\n",
            "          1.9965e-01, -5.6265e-01, -2.3521e-01, -1.2449e-01,  7.5983e-02,\n",
            "          4.1910e-01,  4.7727e-01, -4.3339e-01, -2.8839e-02,  5.3039e-01,\n",
            "          2.8681e-01, -3.1456e-01, -4.5355e-02, -3.4782e-01, -5.6024e-01,\n",
            "         -2.7130e-02,  7.1000e-03,  1.4734e-01, -3.8595e-01, -1.9452e-02,\n",
            "         -1.6854e-01, -2.1339e-01,  1.5089e-01, -5.9381e-02,  7.9870e-02,\n",
            "          6.8614e-03,  1.0731e+00, -2.1785e-01, -4.5960e-02, -1.9407e-01,\n",
            "         -2.5489e-01,  2.8775e-02,  1.7196e-01,  4.5034e-01, -8.9764e-02,\n",
            "         -3.3883e-01,  1.6415e-01, -6.7497e-02, -6.1273e-01, -1.1770e-01,\n",
            "          1.6421e-01,  2.1143e-01, -4.9182e-01, -8.0630e-03, -4.6016e-01,\n",
            "         -2.4081e-01,  2.4578e-01,  3.6238e-01,  4.3722e-01,  4.1992e-01,\n",
            "          5.3694e-01,  9.6174e-02, -8.9640e-02, -2.8170e-01, -8.0827e-02,\n",
            "          3.9255e-01,  3.0680e-01,  3.1028e-01,  4.2743e-02]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "image_path = \"/content/agera.jpeg\"  # Update this path to your local image file\n",
        "image = Image.open(image_path)\n",
        "\n",
        "inputs = processor(image, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs)\n",
        "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated caption:\", caption)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S0YN5pXy09k",
        "outputId": "d161ed9e-985a-4a0d-a5f9-c9271112a080"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated caption: a black sports car driving down a road\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xF9G8Onuzk26"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}